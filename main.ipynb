{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDIM Inversion\n",
    "\n",
    "Notebook written by **Jordan Lin** for CS 188 project.\n",
    "\n",
    "`main.ipynb` is an alternative to `main.py` where I have more flexibility to experiment with my code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False  # True if we are training, False otherwise\n",
    "yaml_path = \"./configs/celeba.yml\"  # Config path for if train = True\n",
    "log_path = \"./logs/run_230226_160439\"  # Model load path for if train = False\n",
    "\n",
    "gpu_num = 0  # For multiple-GPU training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External files edited elsewhere (e.g., PyCharm) are reloaded in Jupyter Notebooks\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from functools import partial\n",
    "\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils import data\n",
    "import torchvision.utils as vutils\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from networks.unet import UNet\n",
    "from runners.diffusion import Diffusion\n",
    "from evaluation.fid import FID\n",
    "\n",
    "import inversion.optimization as oinv\n",
    "import inversion.learning as linv\n",
    "import inversion.hybrid as hinv\n",
    "import inversion.interpolation as iinv\n",
    "\n",
    "import utilities.data as dutils\n",
    "import utilities.math as mutils\n",
    "import utilities.network as nutils\n",
    "import utilities.runner as rutils\n",
    "import utilities.utilities as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f\"cuda:{gpu_num}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_torch_image(image, norm=(0, 1)):\n",
    "    if len(image.shape) == 4:\n",
    "        image = image[0]\n",
    "    if norm is None:\n",
    "        norm = (image.min(), image.max())\n",
    "    image = (image - norm[0]) / (norm[1] - norm[0])\n",
    "    plt.figure(dpi=300)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image.moveaxis(-3, -1).detach().cpu().numpy(), vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_torch_image(image, path):\n",
    "    image = np.clip(image.detach().cpu().moveaxis(0, -1).numpy() * 255, 0, 255).astype(np.uint8)\n",
    "    Image.fromarray(image).save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    config = utils.get_yaml(path=yaml_path)\n",
    "else:\n",
    "    config = utils.get_yaml(path=f\"{log_path}/config.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion = Diffusion(config, device=device)\n",
    "print(f\"Number of parameters: {diffusion.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=logs --port=8008 --load_fast=false --samples_per_plugin images=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if train:\n",
    "    diffusion.train()\n",
    "else:\n",
    "    diffusion.load(path=log_path, name=f\"network_{config.training.num_i}.pth\", ema=False)\n",
    "    diffusion.load(path=log_path, name=f\"ema_{config.training.num_i}.pth\", ema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion.freeze(ema=False)  # Freeze model layers to prevent OOM error during naive inversion\n",
    "diffusion.freeze(ema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Computing the FID score (or even just inception score) takes a very long time as the image generation process takes a while. Thus, currently the default number of sampled images tested is something around $4096$ images, which is not a lot, especially noting that we tend to get lower (i.e., better) FID scores with a larger number of sampled images. The standard is $50000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_generations = diffusion.log_grid(x=\"random\", batch_size=64)\n",
    "display_torch_image(sample_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dutils.get_dataset(name=config.data.dataset, shape=config.data.shape,\n",
    "                                   root=config.data.root, split=\"train\",\n",
    "                                   download=config.data.download)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=config.training.batch_size,\n",
    "                               shuffle=True, num_workers=config.data.num_workers)\n",
    "\n",
    "valid_dataset = dutils.get_dataset(name=config.data.dataset, shape=config.data.shape,\n",
    "                                   root=config.data.root, split=\"valid\",\n",
    "                                   download=config.data.download)\n",
    "valid_loader = data.DataLoader(valid_dataset, batch_size=config.training.batch_size,\n",
    "                               shuffle=True, num_workers=config.data.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = FID(train_loader, valid_loader, config, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_train, fid_valid = fid(diffusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"FID   |   Training: {fid_train.cpu().numpy():.7}   Validation: {fid_valid.cpu().numpy():.7}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_from_dataset(path, loader):\n",
    "    target = torch.from_numpy(np.asarray(Image.open(path)).astype(np.float32) / 255)[:, :, :3]\n",
    "    target = target.moveaxis(-1, 0).unsqueeze(dim=0)\n",
    "    target_find = None\n",
    "    distance_min = torch.tensor(999999, dtype=torch.float32)\n",
    "    for images, label in tqdm(loader):\n",
    "        errors = (target - images).square().mean(dim=(1, 2, 3))\n",
    "        min_i = errors.argmin(dim=0)\n",
    "        if errors[min_i] < distance_min:\n",
    "            distance_min = errors[min_i]\n",
    "            target_find = images[min_i]\n",
    "    return target_find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_existing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_existing:\n",
    "    test_image_1 = find_from_dataset(\"results/validation/celeba_validation_1.png\", valid_loader)\n",
    "    test_image_2 = find_from_dataset(\"results/validation/celeba_validation_2.png\", valid_loader)\n",
    "else:\n",
    "    test_image_1 = next(iter(valid_loader))[0][0]\n",
    "    test_image_2 = next(iter(valid_loader))[0][0]\n",
    "display_torch_image(test_image_1)\n",
    "display_torch_image(test_image_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_1 = torch.randn(*test_image_1.shape, device=device)\n",
    "z_2 = torch.randn(*test_image_2.shape, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_row(images, path, indices=None):\n",
    "    if indices is None:\n",
    "        indices = list(range(len(image)))\n",
    "    images = [images[i] for i in indices]\n",
    "    images = torch.stack(images, dim=0)\n",
    "    images_grid = vutils.make_grid(images, nrow=images.shape[0], padding=2, pad_value=1.0)\n",
    "    if path is not None:\n",
    "        save_torch_image(images_grid, path=path)\n",
    "    display_torch_image(images_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_torch_video(images, path, interval=50, scale=1, codec=\"h264\"):\n",
    "    images = [image.moveaxis(0, -1).numpy() for image in images]\n",
    "    \n",
    "    figure = plt.figure()\n",
    "    axes = plt.Axes(figure, [0.0, 0.0, 1.0, 1.0])\n",
    "    axes.set_axis_off()\n",
    "    figure.add_axes(axes)\n",
    "    figure.set_size_inches(images[0].shape[0] / 100 * scale, images[0].shape[1] / 100 * scale)\n",
    "    \n",
    "    frames = []\n",
    "    for image in images:\n",
    "        frames.append([axes.imshow(image, animated=True, aspect=1)])\n",
    "        \n",
    "    animation_ = animation.ArtistAnimation(figure, frames, interval=50)\n",
    "    animation_.save(path, codec=codec)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "proj_fn_1 = partial(oinv.gradient_inversion, target=test_image_1, diffusion=diffusion,\n",
    "                    optimizer=\"adam\", lr=0.02, num_i=300, criterion=\"ssim\", show_progress=True)\n",
    "proj_fn_2 = partial(oinv.gradient_inversion, target=test_image_2, diffusion=diffusion,\n",
    "                    optimizer=\"adam\", lr=0.02, num_i=300, criterion=\"ssim\", show_progress=True)\n",
    "z_1_trained, x_1_reconstructed = proj_fn_1(z_1.clone(), sequence=True)\n",
    "z_2_trained, x_2_reconstructed = proj_fn_2(z_2.clone(), sequence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [0, 1, 5, 10, 15, 25, 50, 100, 200, 300]\n",
    "indices = [0] + [i + 1 for i in indices]\n",
    "\n",
    "save_row([test_image_1] + x_1_reconstructed, path=None, indices=indices)\n",
    "save_row([test_image_2] + x_2_reconstructed, path=None, indices=indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_row((test_image_1, x_1_reconstructed[-1]), path=\"results/x_1_reconstructed.png\")\n",
    "save_row((test_image_2, x_2_reconstructed[-1]), path=\"results/x_2_reconstructed.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_torch_video(x_1_reconstructed, path=\"results/x_1_reconstructed.webm\", scale=1, codec=\"vp9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_torch_video(x_2_reconstructed, path=\"results/x_2_reconstructed.webm\", scale=1, codec=\"vp9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mixes = iinv.proj_interpolation(z_1_trained[-1].to(device), z_2_trained[-1].to(device),\n",
    "                                  diffusion=diffusion, proj_fn_1=None, proj_fn_2=None,\n",
    "                                  num_t_steps=10, num_alphas=150, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_row((x_1_reconstructed[-1], x_2_reconstructed[-1]), path=\"results/x_mixes.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_torch_video(x_mixes, path=\"results/x_mixes.webm\", scale=1, codec=\"vp9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "transform = transforms.Compose([transforms.CenterCrop((256, 256)), transforms.ToTensor()])\n",
    "\n",
    "lsun_data = datasets.LSUN(root=config.data.root, classes=[\"church_outdoor_train\"], transform=transform)\n",
    "lsun_loader = data.DataLoader(lsun_data, batch_size=config.training.batch_size,\n",
    "                              shuffle=True, num_workers=config.data.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsun_image = next(iter(lsun_loader))[0][0]\n",
    "print(lsun_image.shape)\n",
    "display_torch_image(lsun_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for image, label in tqdm(iter(lsun_loader)):\n",
    "    i += image.shape[0]\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_data = dutils.get_dataset(name=\"miniplaces\", shape=(128, 128),\n",
    "                               root=config.data.root, split=\"train\")\n",
    "mini_loader = data.DataLoader(mini_data, batch_size=config.training.batch_size,\n",
    "                              shuffle=True, num_workers=config.data.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_image = next(iter(mini_loader))[0][0]\n",
    "print(mini_image.shape)\n",
    "display_torch_image(mini_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for image, label in tqdm(iter(mini_loader)):\n",
    "    i += image.shape[0]\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
