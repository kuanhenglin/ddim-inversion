{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDIM Inversion\n",
    "\n",
    "Notebook written by **Jordan Lin**.\n",
    "\n",
    "`main.ipynb` is an alternative to `main.py` where I have more flexibility to experiment with my code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False  # True if we are training, False otherwise\n",
    "yaml_path = \"./configs/celeba.yml\"  # Config path for if train = True\n",
    "run_path = \"run_230311_015229\"\n",
    "log_path = f\"./logs/{run_path}\"  # Model load path for if train = False\n",
    "\n",
    "gpu_num = 0  # For multiple-GPU training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External files edited elsewhere (e.g., PyCharm) are reloaded in Jupyter Notebooks\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from functools import partial\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from networks.unet import UNet\n",
    "from networks.resnet import ResNet\n",
    "from runners.diffusion import Diffusion\n",
    "from evaluation.fid import FID\n",
    "from inversion.learning import NoiseEncoder\n",
    "\n",
    "import inversion.optimization as oinv\n",
    "import inversion.learning as linv\n",
    "import inversion.hybrid as hinv\n",
    "import inversion.interpolation as iinv\n",
    "\n",
    "import editing.classification as eclass\n",
    "\n",
    "import utilities.data as dutils\n",
    "import utilities.math as mutils\n",
    "import utilities.network as nutils\n",
    "import utilities.runner as rutils\n",
    "import utilities.utilities as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f\"cuda:{gpu_num}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_torch_image(image, norm=(0, 1), dpi=72):\n",
    "    if len(image.shape) == 4:\n",
    "        image = image[0]\n",
    "    if norm is None:\n",
    "        norm = (image.min(), image.max())\n",
    "    image = (image - norm[0]) / (norm[1] - norm[0])\n",
    "    plt.figure(dpi=dpi)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image.moveaxis(-3, -1).detach().cpu().numpy(), vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_torch_image(image, path):\n",
    "    image = np.clip(image.detach().cpu().moveaxis(0, -1).numpy() * 255, 0, 255).astype(np.uint8)\n",
    "    Image.fromarray(image).save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    config = utils.get_yaml(path=yaml_path)\n",
    "else:\n",
    "    config = utils.get_yaml(path=f\"{log_path}/config.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data=Namespace(dataset='celeba', root='~/.torch/datasets', shape=[3, 64, 64], shape_original=[3, 218, 178], num_train=162770, num_valid=19867, num_test=19962, random_flip=True, zero_center=True, clamp=True, flip_horizontal=0.5, flip_vertical=0.0, num_workers=4, download=True), network=Namespace(hidden_channels=32, num_blocks=2, channel_mults=[1, 2, 2, 2, 4], attention_sizes=[16], embed_channels=128, dropout=0.1, num_groups=8, ema=0.9995, do_conv_sample=True), diffusion=Namespace(beta_schedule='linear', beta_start=0.0001, beta_end=0.02, num_t=1000, num_t_steps=50, eta=0.0), training=Namespace(batch_size=64, log_batch_size=64, criterion='l1', num_i=72000, log_frequency=300, save_frequency=6000, tensorboard=True), evaluation=Namespace(batch_size=64, num_batches=64), optimizer=Namespace(name='adam', learning_rate=0.0002, weight_decay=0.0, beta_1=0.9, amsgrad=False, epsilon=1e-08, gradient_clip=1.0))\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 4935747\n"
     ]
    }
   ],
   "source": [
    "diffusion = Diffusion(config, device=device)\n",
    "print(f\"Number of parameters: {diffusion.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=logs --port=8008 --load_fast=false --samples_per_plugin images=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if train:\n",
    "    diffusion.train()\n",
    "else:\n",
    "    diffusion.load(path=log_path, name=f\"network_{config.training.num_i}.pth\", ema=False)\n",
    "    diffusion.load(path=log_path, name=f\"ema_{config.training.num_i}.pth\", ema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion.freeze(ema=False)  # Freeze model layers to prevent OOM error during naive inversion\n",
    "diffusion.freeze(ema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sample_generations = diffusion.log_grid(x=\"random\", num_t_steps=10, batch_size=64)\n",
    "display_torch_image(sample_generations, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dutils.get_dataset(name=config.data.dataset, shape=config.data.shape,\n",
    "                                   root=config.data.root, split=\"train\",\n",
    "                                   download=config.data.download)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=True,\n",
    "                               num_workers=config.data.num_workers)\n",
    "\n",
    "valid_dataset = dutils.get_dataset(name=config.data.dataset, shape=config.data.shape,\n",
    "                                   root=config.data.root, split=\"valid\",\n",
    "                                   download=config.data.download)\n",
    "valid_loader = data.DataLoader(valid_dataset, batch_size=128, shuffle=True,\n",
    "                               num_workers=config.data.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CelebA Facial Attributes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "celeba_binary = [\"5_o_clock_shadow\", \"arched_eyebrows\", \"attractive\", \"bags_under_eyes\", \"bald\",\n",
    "                 \"bangs\", \"big_lips\", \"big_nose\", \"black_hair\", \"blond_hair\", \"blurry\",\n",
    "                 \"brown_hair\", \"bushy_eyebrows\", \"chubby\", \"double_chin\", \"eyeglasses\", \"goatee\",\n",
    "                 \"gray_hair\", \"heavy_makeup\", \"high_cheekbones\", \"male\", \"mouth_slightly_open\",\n",
    "                 \"mustache\", \"narrow_eyes\", \"no_beard\", \"oval_face\", \"pale_skin\", \"pointy_nose\",\n",
    "                 \"receding_hairline\", \"rosy_cheeks\", \"sideburns\", \"smiling\", \"straight_hair\",\n",
    "                 \"wavy_hair\", \"wearing_earrings\", \"wearing_hat\", \"wearing_lipstick\",\n",
    "                 \"wearing_necklace\", \"wearing_necktie\", \"young\"]\n",
    "celeba_targets = [\"attractive\", \"eyeglasses\", \"male\", \"smiling\", \"young\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e8515e7cfe4b139ce19ca119b79472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738f6c9ef59442f28728baea09a55e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/156 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "celeba_train_counts = eclass.get_class_counts(train_loader, celeba_binary, None, ratio=True)\n",
    "celeba_valid_counts = eclass.get_class_counts(valid_loader, celeba_binary, None, ratio=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'5_o_clock_shadow': 0.11167291269890028, 'arched_eyebrows': 0.2658843767278983, 'attractive': 0.5136265896664004, 'bags_under_eyes': 0.20446028137863242, 'bald': 0.022811328868956197, 'bangs': 0.15165571051176507, 'big_lips': 0.24091048719051422, 'big_nose': 0.23555323462554525, 'black_hair': 0.23902439024390243, 'blond_hair': 0.149087669718007, 'blurry': 0.05137310315168643, 'brown_hair': 0.20391964121152548, 'bushy_eyebrows': 0.1436751244086748, 'chubby': 0.05768261964735517, 'double_chin': 0.04651348528598636, 'eyeglasses': 0.0646372181605947, 'goatee': 0.06350678872028015, 'gray_hair': 0.04236652945874547, 'heavy_makeup': 0.38431529151563554, 'high_cheekbones': 0.45244823984763777, 'male': 0.4193708914419119, 'mouth_slightly_open': 0.4821895926767832, 'mustache': 0.040806045340050376, 'narrow_eyes': 0.11592431037660503, 'no_beard': 0.8341770596547275, 'oval_face': 0.28322786754315904, 'pale_skin': 0.04303618602936659, 'pointy_nose': 0.2755176015236223, 'receding_hairline': 0.08011304294403146, 'rosy_cheeks': 0.06466179271364501, 'sideburns': 0.05625115193217423, 'smiling': 0.47969527554217606, 'straight_hair': 0.2085580880997727, 'wavy_hair': 0.31935860416538675, 'wearing_earrings': 0.18653314492842663, 'wearing_hat': 0.04938870799287338, 'wearing_lipstick': 0.4696012778767586, 'wearing_necklace': 0.12142286662161332, 'wearing_necktie': 0.07304785894206549, 'young': 0.7789396080358788}\n"
     ]
    }
   ],
   "source": [
    "print(celeba_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 273701\n"
     ]
    }
   ],
   "source": [
    "# This is basically the original CIFAR-10 ResNet-20 architecture\n",
    "resnet = ResNet(in_shape=config.data.shape, num_classes=len(celeba_targets),\n",
    "                filters=[[16, 16], [32, 32], [64, 64]], kernels=[[3, 3], [3, 3], [3, 3]],\n",
    "                repeats=[3, 3, 3], in_kernel=5, in_stride=2,\n",
    "                in_max_pool_kernel=1, in_max_pool_stride=1)\n",
    "resnet.to(device)\n",
    "print(f\"Number of parameters: {utils.get_size(resnet)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62eabf2e2204642ac43986b714f6167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1500]     Training: 88.357% (0.2591)   Validation: 87.487% (0.2694)\n",
      "[3000]     Training: 87.472% (0.2729)   Validation: 86.922% (0.2834)\n",
      "[4500]     Training: 90.008% (0.2252)   Validation: 89.309% (0.2368)\n"
     ]
    }
   ],
   "source": [
    "eclass.classification(resnet, train_loader, valid_loader, celeba_binary, celeba_targets,\n",
    "                      train_weights=celeba_train_counts, valid_weights=celeba_valid_counts,\n",
    "                      i_max=30000, i_print=(30000 // 20), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "resnet.eval()\n",
    "celeba_target_indices = eclass.get_class_indices(celeba_binary, celeba_targets)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_test = next(iter(valid_loader))\n",
    "    sample_outputs = torch.sigmoid(resnet(sample_test[0].to(device)))\n",
    "    \n",
    "display_torch_image(sample_test[0][1])\n",
    "print(torch.stack((sample_test[1][0][celeba_target_indices],\n",
    "                   sample_outputs[0].cpu()), dim=0).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Many\n",
    "\n",
    "This can be useful for training encoders for inversion later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_many(diffusion, num_t_steps, batch_size, num_batches, save_frequency, save_noise=True):\n",
    "    samples = []\n",
    "    if save_noise:\n",
    "        noises = []\n",
    "    total = num_batches * batch_size\n",
    "    \n",
    "    os.mkdir(f\"./samples/{run_path}\")\n",
    "        \n",
    "    for i in tqdm(range(num_batches)):\n",
    "        noise = torch.randn(batch_size, *config.data.shape)\n",
    "        sample = diffusion.sample(x=noise.to(device), num_t_steps=num_t_steps, sequence=False)\n",
    "        samples.append(sample.detach().cpu())\n",
    "        if save_noise:\n",
    "            noises.append(noise.detach().cpu())\n",
    "        \n",
    "        if (i + 1) % save_frequency == 0:\n",
    "            num_sampled = (i + 1) * batch_size\n",
    "            file_index = str(num_sampled).zfill(len(str(total)))\n",
    "            \n",
    "            samples = torch.cat(samples, dim=0)\n",
    "            torch.save(samples, f\"./samples/{run_path}/samples_{file_index}.pth\")\n",
    "            samples = []\n",
    "            if save_noise:\n",
    "                noises = torch.cat(noises, dim=0)\n",
    "                torch.save(noises, f\"./samples/{run_path}/noises_{file_index}.pth\")\n",
    "                noises = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_many(diffusion, num_t_steps=10, batch_size=64, num_batches=4096, save_frequency=64,\n",
    "            save_noise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_loader = dutils.get_loader_samples(batch_size=64, root=f\"./samples/{run_path}\",\n",
    "                                          stop_iteration=True)  # Iterable custom loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### FID\n",
    "\n",
    "Computing the FID score (or even just inception score) takes a very long time as the image generation process takes a while. Thus, currently the default number of sampled images tested is something around $4096$ images, which is not a lot, especially noting that we tend to get lower (i.e., better) FID scores with a larger number of sampled images. The standard is $50000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = FID(train_loader, valid_loader, config, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_train, fid_valid = fid(diffusion, batch_size=100, num_batches=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"FID   |   Training: {fid_train.cpu().numpy():.7}   \"\n",
    "      f\"Validation: {fid_valid.cpu().numpy():.7}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_row(images, path, indices=None):\n",
    "    if indices is None:\n",
    "        indices = list(range(len(images)))\n",
    "    images = [images[i] for i in indices]\n",
    "    images = torch.stack(images, dim=0)\n",
    "    images_grid = vutils.make_grid(images, nrow=images.shape[0], padding=2, pad_value=1.0)\n",
    "    if path is not None:\n",
    "        save_torch_image(images_grid, path=path)\n",
    "    display_torch_image(images_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_torch_video(images, path, interval=50, scale=1, codec=\"h264\"):\n",
    "    images = [image.moveaxis(0, -1).numpy() for image in images]\n",
    "    \n",
    "    figure = plt.figure()\n",
    "    axes = plt.Axes(figure, [0.0, 0.0, 1.0, 1.0])\n",
    "    axes.set_axis_off()\n",
    "    figure.add_axes(axes)\n",
    "    figure.set_size_inches(images[0].shape[0] / 100 * scale, images[0].shape[1] / 100 * scale)\n",
    "    \n",
    "    frames = []\n",
    "    for image in images:\n",
    "        frames.append([axes.imshow(image, animated=True, aspect=1)])\n",
    "        \n",
    "    animation_ = animation.ArtistAnimation(figure, frames, interval=50)\n",
    "    animation_.save(path, codec=codec)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, resize=None):\n",
    "    image = torch.from_numpy(np.asarray(Image.open(path)).astype(np.float32) / 255)[:, :, :3]\n",
    "    image = image.moveaxis(-1, 0).unsqueeze(dim=0)\n",
    "    if resize is not None:\n",
    "        image = F.interpolate(image, resize, mode=\"area\")\n",
    "        # image = transforms.functional.resize(image, resize, transforms.InterpolationMode.BILINEAR)\n",
    "    return image[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_from_dataset(path, loader):\n",
    "    target = load_image(path)\n",
    "    target_find = None\n",
    "    distance_min = torch.tensor(999999, dtype=torch.float32)\n",
    "    for images, label in tqdm(loader):\n",
    "        errors = (target - images).square().mean(dim=(1, 2, 3))\n",
    "        min_i = errors.argmin(dim=0)\n",
    "        if errors[min_i] < distance_min:\n",
    "            distance_min = errors[min_i]\n",
    "            target_find = images[min_i]\n",
    "    return target_find"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_existing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(load_existing) in (list, tuple):\n",
    "    test_image_1 = load_image(load_existing[0], resize=(64, 64))\n",
    "    test_image_2 = load_image(load_existing[1], resize=(64, 64))\n",
    "elif load_existing:\n",
    "    test_image_1 = find_from_dataset(\"results/validation/celeba_validation_1.png\", valid_loader)\n",
    "    test_image_2 = find_from_dataset(\"results/validation/celeba_validation_2.png\", valid_loader)\n",
    "else:\n",
    "    test_image_1 = next(iter(valid_loader))[0][0]\n",
    "    test_image_2 = next(iter(valid_loader))[0][0]\n",
    "display_torch_image(test_image_1, dpi=72)\n",
    "display_torch_image(test_image_2, dpi=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_1 = torch.randn(*test_image_1.shape, device=device)\n",
    "z_2 = torch.randn(*test_image_2.shape, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "proj_fn_1 = partial(oinv.gradient_inversion, target=test_image_1, diffusion=diffusion,\n",
    "                    optimizer=\"adam\", lr=0.02, num_i=300, criterion=\"psnr\", show_progress=True)\n",
    "proj_fn_2 = partial(oinv.gradient_inversion, target=test_image_2, diffusion=diffusion,\n",
    "                    optimizer=\"adam\", lr=0.02, num_i=300, criterion=\"psnr\", show_progress=True)\n",
    "z_1_trained, x_1_reconstructed = proj_fn_1(z_1.clone(), sequence=True)\n",
    "z_2_trained, x_2_reconstructed = proj_fn_2(z_2.clone(), sequence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_row((test_image_1, x_1_reconstructed[-1]), path=\"results/x_1_reconstructed.png\")\n",
    "save_row((test_image_2, x_2_reconstructed[-1]), path=\"results/x_2_reconstructed.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_torch_video(x_1_reconstructed, path=\"results/x_1_reconstructed.webm\", scale=1, codec=\"vp9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_torch_video(x_2_reconstructed, path=\"results/x_2_reconstructed.webm\", scale=1, codec=\"vp9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = False\n",
    "\n",
    "if anime:\n",
    "    indices = [0, 1, 5, 10, 15, 25, 50, 100, 200, 300]\n",
    "    indices = [0] + [i + 1 for i in indices]\n",
    "\n",
    "    save_row([test_image_1] + x_1_reconstructed, path=\"anime_inversion_1.png\", indices=indices)\n",
    "    save_row([test_image_2] + x_2_reconstructed, path=\"anime_inversion_2.png\", indices=indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_args = {\"hidden_channels\": 16, \"num_blocks\": 2, \"channel_mults\": [1, 2, 2, 4],\n",
    "                \"attention_sizes\": [], \"time_embed_channels\": None, \"dropout\": 0.1,\n",
    "                \"num_groups\": 8, \"do_conv_sample\": True, \"out_conv_zero\": False}\n",
    "encoder = NoiseEncoder(config, network_args=encoder_args, loss_type=\"reconstruction\",\n",
    "                       diffusion=diffusion, device=device)\n",
    "print(f\"Number of parameters: {encoder.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_args = {\"num_t_steps\": 10}\n",
    "optimizer_args = {\"name\": \"adam\", \"learning_rate\": 0.0002, \"weight_decay\": 0.0, \"beta_1\": 0.9,\n",
    "                  \"amsgrad\": False, \"epsilon\": 1e-7}\n",
    "encoder.train(diffusion_args, optimizer_args, batch_size=8, num_i=6000,\n",
    "              z_criterion=\"l2\", x_criterion=\"psnr\", loader=f\"./samples/{run_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mixes = iinv.proj_interpolation(z_1_trained[-1].to(device), z_2_trained[-1].to(device),\n",
    "                                  diffusion=diffusion, proj_fn_1=None, proj_fn_2=None,\n",
    "                                  num_t_steps=10, num_alphas=150, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_row((x_1_reconstructed[-1], x_2_reconstructed[-1]), path=\"results/x_mixes.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_row(x_mixes, indices=[0, 25, 49, 75, 99, 124, 149], path=\"results/x_mixes_.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_torch_video(x_mixes, path=\"results/x_mixes.webm\", scale=1, codec=\"vp9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "transform = transforms.Compose([transforms.CenterCrop((256, 256)), transforms.ToTensor()])\n",
    "\n",
    "lsun_data = datasets.LSUN(root=config.data.root, classes=[\"church_outdoor_train\"], transform=transform)\n",
    "lsun_loader = data.DataLoader(lsun_data, batch_size=config.training.batch_size,\n",
    "                              shuffle=True, num_workers=config.data.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsun_image = next(iter(lsun_loader))[0][0]\n",
    "print(lsun_image.shape)\n",
    "display_torch_image(lsun_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for image, label in tqdm(iter(lsun_loader)):\n",
    "    i += image.shape[0]\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_data = dutils.get_dataset(name=\"miniplaces\", shape=(128, 128),\n",
    "                               root=config.data.root, split=\"train\")\n",
    "mini_loader = data.DataLoader(mini_data, batch_size=config.training.batch_size,\n",
    "                              shuffle=True, num_workers=config.data.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_image = next(iter(mini_loader))[0][0]\n",
    "print(mini_image.shape)\n",
    "display_torch_image(mini_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for image, label in tqdm(iter(mini_loader)):\n",
    "    i += image.shape[0]\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
