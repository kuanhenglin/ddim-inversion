{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDIM Inversion\n",
    "\n",
    "Notebook written by **Jordan Lin**.\n",
    "\n",
    "`main.ipynb` is an alternative to `main.py` where I have more flexibility to experiment with my code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False  # True if we are training, False otherwise\n",
    "yaml_path = \"./configs/celeba.yml\"  # Config path for if train = True\n",
    "run_path = \"run_230226_160439\"\n",
    "log_path = f\"./logs/{run_path}\"  # Model load path for if train = False\n",
    "\n",
    "gpu_num = 0  # For multiple-GPU training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External files edited elsewhere (e.g., PyCharm) are reloaded in Jupyter Notebooks\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from functools import partial\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from networks.unet import UNet\n",
    "from networks.resnet import ResNet\n",
    "from runners.diffusion import Diffusion\n",
    "from evaluation.fid import FID\n",
    "from inversion.learning import NoiseEncoder\n",
    "from editing.attribute import AttributeClassifier\n",
    "\n",
    "import inversion.optimization as oinv\n",
    "import inversion.learning as linv\n",
    "import inversion.hybrid as hinv\n",
    "import inversion.interpolation as iinv\n",
    "\n",
    "import editing.classification as eclass\n",
    "\n",
    "import utilities.data as dutils\n",
    "import utilities.math as mutils\n",
    "import utilities.network as nutils\n",
    "import utilities.runner as rutils\n",
    "import utilities.utilities as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f\"cuda:{gpu_num}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_torch_image(image, norm=(0, 1), dpi=72, title=None, save=None):\n",
    "    if len(image.shape) == 4:\n",
    "        image = image[0]\n",
    "    if norm is None:\n",
    "        norm = (image.min(), image.max())\n",
    "    image = (image - norm[0]) / (norm[1] - norm[0])\n",
    "    plt.figure(dpi=dpi)\n",
    "    plt.axis(\"off\")\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.imshow(image.moveaxis(-3, -1).detach().cpu().numpy(), vmin=0, vmax=1)\n",
    "    if save is not None:\n",
    "        save_torch_image(image, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_torch_sequence(images, normalize=False, indices=None, sort=True, titles=True,\n",
    "                           dpi=72, cmap=None, **kwargs):\n",
    "    if type(images) in (tuple, list):\n",
    "        images = torch.stack(images, dim=0)\n",
    "    if type(indices) == int:\n",
    "        indices = np.random.choice(len(images), indices)\n",
    "        if sort:\n",
    "            indices.sort()\n",
    "    elif indices is None:\n",
    "        indices = list(range(len(images)))\n",
    "    images_show = images[indices]\n",
    "\n",
    "    figure, axes = plt.subplots(1, len(images_show), figsize=(20, 20), dpi=dpi)\n",
    "    for i in range(images_show.shape[0]):\n",
    "        image = images_show[i].moveaxis(0, -1)\n",
    "        if type(titles) in (tuple, list):\n",
    "            axes[i].title.set_text(titles[i])\n",
    "        elif titles:\n",
    "            axes[i].title.set_text(f\"$i = {idxs[i]}$\")\n",
    "        axes[i].axis(\"off\")\n",
    "        if normalize:\n",
    "            image = (image - image.min()) / (image.max() - image.min())\n",
    "        axes[i].imshow(image, cmap=cmap, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_torch_image(image, path):\n",
    "    image = np.clip(image.detach().cpu().moveaxis(0, -1).numpy() * 255, 0, 255).astype(np.uint8)\n",
    "    Image.fromarray(image).save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    config = utils.get_yaml(path=yaml_path)\n",
    "else:\n",
    "    config = utils.get_yaml(path=f\"{log_path}/config.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion = Diffusion(config, device=device)\n",
    "print(f\"Number of parameters: {diffusion.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=logs --port=8008 --load_fast=false --samples_per_plugin images=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if train:\n",
    "    diffusion.train()\n",
    "else:\n",
    "    diffusion.load(path=log_path, name=f\"network_{config.training.num_i}.pth\", ema=False)\n",
    "    diffusion.load(path=log_path, name=f\"ema_{config.training.num_i}.pth\", ema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion.freeze(ema=False)  # Freeze model layers to prevent OOM error during naive inversion\n",
    "diffusion.freeze(ema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_t_steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sample_generations = diffusion.log_grid(x=\"random\", num_t_steps=num_t_steps, batch_size=64)\n",
    "display_torch_image(sample_generations, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dutils.get_dataset(name=config.data.dataset, shape=config.data.shape,\n",
    "                                   root=config.data.root, split=\"train\",\n",
    "                                   download=config.data.download)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=True,\n",
    "                               num_workers=config.data.num_workers)\n",
    "\n",
    "valid_dataset = dutils.get_dataset(name=config.data.dataset, shape=config.data.shape,\n",
    "                                   root=config.data.root, split=\"valid\",\n",
    "                                   download=config.data.download)\n",
    "valid_loader = data.DataLoader(valid_dataset, batch_size=128, shuffle=True,\n",
    "                               num_workers=config.data.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### FID\n",
    "\n",
    "Computing the FID score (or even just inception score) takes a very long time as the image generation process takes a while. Thus, currently the default number of sampled images tested is something around $4096$ images, which is not a lot, especially noting that we tend to get lower (i.e., better) FID scores with a larger number of sampled images. The standard is $50000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = FID(train_loader, valid_loader, config, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_train, fid_valid = fid(diffusion, batch_size=100, num_batches=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"FID   |   Training: {fid_train.cpu().numpy():.7}   \"\n",
    "      f\"Validation: {fid_valid.cpu().numpy():.7}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Many\n",
    "\n",
    "This can be useful for training encoders for inversion later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_many(diffusion, num_t_steps, batch_size, num_batches, save_frequency, i_start=0,\n",
    "                save_noise=True, exist_ok=True):\n",
    "    samples = []\n",
    "    if save_noise:\n",
    "        noises = []\n",
    "    total = num_batches * batch_size\n",
    "    \n",
    "    path = f\"./samples/{run_path}_t{num_t_steps}\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except:\n",
    "        if exist_ok:\n",
    "            print(\"Directory {path} already exists, continuing.\")\n",
    "        else:\n",
    "            raise Error(\"Directory {path} already exists.\")\n",
    "        \n",
    "    for i in tqdm(range(i_start, num_batches)):\n",
    "        noise = torch.randn(batch_size, *config.data.shape)\n",
    "        sample = diffusion.sample(x=noise.to(device), num_t_steps=num_t_steps, sequence=False)\n",
    "        samples.append(sample.detach().cpu())\n",
    "        if save_noise:\n",
    "            noises.append(noise.detach().cpu())\n",
    "        \n",
    "        if (i + 1) % save_frequency == 0:\n",
    "            num_sampled = (i + 1) * batch_size\n",
    "            file_index = str(num_sampled).zfill(len(str(total)))\n",
    "            \n",
    "            samples = torch.cat(samples, dim=0)\n",
    "            torch.save(samples, f\"./samples/{run_path}_t{num_t_steps}/samples_{file_index}.pth\")\n",
    "            samples = []\n",
    "            if save_noise:\n",
    "                noises = torch.cat(noises, dim=0)\n",
    "                torch.save(noises, f\"./samples/{run_path}_t{num_t_steps}/noises_{file_index}.pth\")\n",
    "                noises = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_many(diffusion, num_t_steps=num_t_steps, batch_size=64, num_batches=4096, save_frequency=64,\n",
    "            i_start=2816, save_noise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_loader = dutils.get_loader_samples(  # Iterable custom loader\n",
    "    batch_size=128, root=f\"./samples/{run_path}_t{num_t_steps}\", stop_iteration=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_row(images, path, indices=None):\n",
    "    if indices is None:\n",
    "        indices = list(range(len(images)))\n",
    "    images = [images[i] for i in indices]\n",
    "    images = torch.stack(images, dim=0)\n",
    "    images_grid = vutils.make_grid(images, nrow=images.shape[0], padding=2, pad_value=1.0)\n",
    "    if path is not None:\n",
    "        save_torch_image(images_grid, path=path)\n",
    "    display_torch_image(images_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_torch_video(images, path, interval=50, scale=1, codec=\"h264\"):\n",
    "    images = [image.moveaxis(0, -1).numpy() for image in images]\n",
    "    \n",
    "    figure = plt.figure()\n",
    "    axes = plt.Axes(figure, [0.0, 0.0, 1.0, 1.0])\n",
    "    axes.set_axis_off()\n",
    "    figure.add_axes(axes)\n",
    "    figure.set_size_inches(images[0].shape[0] / 100 * scale, images[0].shape[1] / 100 * scale)\n",
    "    \n",
    "    frames = []\n",
    "    for image in images:\n",
    "        frames.append([axes.imshow(image, animated=True, aspect=1)])\n",
    "        \n",
    "    animation_ = animation.ArtistAnimation(figure, frames, interval=50)\n",
    "    animation_.save(path, codec=codec)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, resize=None):\n",
    "    image = torch.from_numpy(np.asarray(Image.open(path)).astype(np.float32) / 255)[:, :, :3]\n",
    "    image = image.moveaxis(-1, 0).unsqueeze(dim=0)\n",
    "    if resize is not None:\n",
    "        image = F.interpolate(image, resize, mode=\"area\")\n",
    "        # image = transforms.functional.resize(image, resize, transforms.InterpolationMode.BILINEAR)\n",
    "    return image[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_from_dataset(path, loader):\n",
    "    target = load_image(path)\n",
    "    target_find = None\n",
    "    distance_min = torch.tensor(999999, dtype=torch.float32)\n",
    "    for images, label in tqdm(loader):\n",
    "        errors = (target - images).square().mean(dim=(1, 2, 3))\n",
    "        min_i = errors.argmin(dim=0)\n",
    "        if errors[min_i] < distance_min:\n",
    "            distance_min = errors[min_i]\n",
    "            target_find = images[min_i]\n",
    "    return target_find"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_existing = (\"./.dsu/nick_1.png\", \"./.dsu/emily_1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(load_existing) in (list, tuple):\n",
    "    test_image_1 = load_image(load_existing[0], resize=(64, 64))\n",
    "    test_image_2 = load_image(load_existing[1], resize=(64, 64))\n",
    "elif load_existing:\n",
    "    test_image_1 = find_from_dataset(\"results/validation/celeba_validation_1.png\", valid_loader)\n",
    "    test_image_2 = find_from_dataset(\"results/validation/celeba_validation_2.png\", valid_loader)\n",
    "else:\n",
    "    test_image_1 = next(iter(valid_loader))[0][0]\n",
    "    test_image_2 = next(iter(valid_loader))[0][0]\n",
    "display_torch_image(test_image_1, dpi=72)\n",
    "display_torch_image(test_image_2, dpi=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_1 = torch.randn(*test_image_1.shape, device=device)\n",
    "z_2 = torch.randn(*test_image_2.shape, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "proj_fn_1 = partial(oinv.gradient_inversion, target=test_image_1, diffusion=diffusion,\n",
    "                    optimizer=\"adam\", lr=0.02, num_i=300, criterion=\"psnr\", show_progress=True)\n",
    "proj_fn_2 = partial(oinv.gradient_inversion, target=test_image_2, diffusion=diffusion,\n",
    "                    optimizer=\"adam\", lr=0.02, num_i=300, criterion=\"psnr\", show_progress=True)\n",
    "z_1_trained, x_1_reconstructed = proj_fn_1(z_1.clone(), num_t_steps=10, sequence=True)\n",
    "z_2_trained, x_2_reconstructed = proj_fn_2(z_2.clone(), num_t_steps=10, sequence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_row((test_image_1, x_1_reconstructed[-1]), path=\"results/x_1_reconstructed.png\")\n",
    "save_row((test_image_2, x_2_reconstructed[-1]), path=\"results/x_2_reconstructed.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_torch_video(x_1_reconstructed, path=\"results/x_1_reconstructed.webm\", scale=1, codec=\"vp9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_torch_video(x_2_reconstructed, path=\"results/x_2_reconstructed.webm\", scale=1, codec=\"vp9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = False\n",
    "\n",
    "if anime:\n",
    "    indices = [0, 1, 5, 10, 15, 25, 50, 100, 200, 300]\n",
    "    indices = [0] + [i + 1 for i in indices]\n",
    "\n",
    "    save_row([test_image_1] + x_1_reconstructed, path=\"anime_inversion_1.png\", indices=indices)\n",
    "    save_row([test_image_2] + x_2_reconstructed, path=\"anime_inversion_2.png\", indices=indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_args = {\"hidden_channels\": 16, \"num_blocks\": 2, \"channel_mults\": [1, 2, 2, 4],\n",
    "                \"attention_sizes\": [], \"time_embed_channels\": None, \"dropout\": 0.1,\n",
    "                \"num_groups\": 8, \"do_conv_sample\": True, \"out_conv_zero\": False}\n",
    "encoder = NoiseEncoder(config, network_args=encoder_args, loss_type=\"reconstruction\",\n",
    "                       diffusion=diffusion, device=device)\n",
    "print(f\"Number of parameters: {encoder.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_args = {\"num_t_steps\": 10}\n",
    "optimizer_args = {\"name\": \"adam\", \"learning_rate\": 0.0002, \"weight_decay\": 0.0, \"beta_1\": 0.9,\n",
    "                  \"amsgrad\": False, \"epsilon\": 1e-7}\n",
    "encoder.train(diffusion_args, optimizer_args, batch_size=8, num_i=6000,\n",
    "              z_criterion=\"l2\", x_criterion=\"psnr\", loader=f\"./samples/{run_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mixes = iinv.proj_interpolation(z_1_trained[-1].to(device), z_2_trained[-1].to(device),\n",
    "                                  diffusion=diffusion, proj_fn_1=None, proj_fn_2=None,\n",
    "                                  num_t_steps=10, num_alphas=150, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_row((x_1_reconstructed[-1], x_2_reconstructed[-1]), path=\"results/x_mixes.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_row(x_mixes, indices=[0, 25, 49, 75, 99, 124, 149], path=\"results/x_mixes_.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_torch_video(x_mixes, path=\"results/x_mixes.webm\", scale=1, codec=\"vp9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Feature Editing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CelebA Facial Attributes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_display(names, results, targets):\n",
    "    attributes = zip(names, results, targets)\n",
    "    for name, result, target in attributes:\n",
    "        print(f\"{name}: {float(result):.7} ({target})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "celeba_binary = [\"5_o_clock_shadow\", \"arched_eyebrows\", \"attractive\", \"bags_under_eyes\", \"bald\",\n",
    "                 \"bangs\", \"big_lips\", \"big_nose\", \"black_hair\", \"blond_hair\", \"blurry\",\n",
    "                 \"brown_hair\", \"bushy_eyebrows\", \"chubby\", \"double_chin\", \"eyeglasses\", \"goatee\",\n",
    "                 \"gray_hair\", \"heavy_makeup\", \"high_cheekbones\", \"male\", \"mouth_slightly_open\",\n",
    "                 \"mustache\", \"narrow_eyes\", \"no_beard\", \"oval_face\", \"pale_skin\", \"pointy_nose\",\n",
    "                 \"receding_hairline\", \"rosy_cheeks\", \"sideburns\", \"smiling\", \"straight_hair\",\n",
    "                 \"wavy_hair\", \"wearing_earrings\", \"wearing_hat\", \"wearing_lipstick\",\n",
    "                 \"wearing_necklace\", \"wearing_necktie\", \"young\"]\n",
    "celeba_targets = celeba_binary\n",
    "# celeba_targets = [\"attractive\", \"eyeglasses\", \"male\", \"smiling\", \"young\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count training and validation data distribution to balance data during training\n",
    "\n",
    "celeba_train_counts = eclass.get_class_counts(train_loader, celeba_binary, None, ratio=True)\n",
    "celeba_valid_counts = eclass.get_class_counts(valid_loader, celeba_binary, None, ratio=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(celeba_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_resnet = \"./logs/celeba_classification/resnet_c40_small.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is basically the original CIFAR-10 ResNet-20 architecture\n",
    "resnet = ResNet(in_shape=config.data.shape, num_classes=len(celeba_targets),\n",
    "                filters=[[16, 16], [32, 32], [64, 64]], kernels=[[3, 3], [3, 3], [3, 3]],\n",
    "                repeats=[3, 3, 3], in_kernel=5, in_stride=2,\n",
    "                in_max_pool_kernel=1, in_max_pool_stride=1)\n",
    "resnet.to(device)\n",
    "if load_resnet is not None:\n",
    "    resnet.load_state_dict(torch.load(load_resnet, map_location=device))\n",
    "\n",
    "print(f\"Number of parameters: {utils.get_size(resnet)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if load_resnet is None:\n",
    "    eclass.classification(resnet, train_loader, valid_loader, celeba_binary, celeba_targets,\n",
    "                          train_weights=celeba_train_counts, valid_weights=celeba_valid_counts,\n",
    "                          i_max=60000, i_print=(60000 // 50), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check on validation dataset\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "resnet.eval()\n",
    "celeba_target_indices = eclass.get_class_indices(celeba_binary, celeba_targets)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_test = next(iter(valid_loader))\n",
    "    sample_outputs = torch.sigmoid(resnet(sample_test[0].to(device)))\n",
    "    \n",
    "display_torch_image(sample_test[0][1])\n",
    "attribute_display(celeba_targets, list(sample_outputs[0].cpu()),\n",
    "                  list(sample_test[1][0][celeba_target_indices]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_classifier = AttributeClassifier(\n",
    "    in_shape=config.data.shape, weight_decay=0.001, diffusion=diffusion, resnet=resnet,\n",
    "    resnet_targets=celeba_targets, target=\"wavy_hair\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_classifier.train(config, diffusion_args=None, loader=sample_loader,\n",
    "                           batch_size=128, num_i=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = attribute_classifier.get_direction(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check to make sure the classifier is working\n",
    "\n",
    "sample_test = next(iter(sample_loader))\n",
    "display_torch_image(sample_test[0][0])\n",
    "attribute_classifier.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_resnet = (resnet(sample_test[0].to(device))[:, attribute_classifier.target_index] >= 0)\\\n",
    "                    .to(torch.int32)\n",
    "    sample_classifier = attribute_classifier(sample_test[1].to(device))\n",
    "\n",
    "print(f\"{sample_classifier[0].item()} ({sample_resnet[0].item()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(3, 64, 64).to(device)\n",
    "# z = z_1_trained[-1].to(device)\n",
    "x = diffusion.sample(x=z, num_t_steps=num_t_steps)\n",
    "display_torch_image(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 3.0\n",
    "alphas = np.linspace(-scale, scale, 9)\n",
    "images = []\n",
    "titles = []\n",
    "with torch.no_grad():\n",
    "    for alpha in tqdm(alphas):\n",
    "        z_new = z + alpha * u\n",
    "        x = diffusion.sample(x=z_new, num_t_steps=num_t_steps)\n",
    "        images.append(x.detach().cpu()[0])\n",
    "        titles.append(fr\"$\\alpha = {alpha}$\")\n",
    "display_torch_sequence(images, titles=titles, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_torch_image(u, norm=None, save=\"./results/celeba_editing_wavy_u.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
